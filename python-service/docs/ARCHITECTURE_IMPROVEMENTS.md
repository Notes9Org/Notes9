# Architecture Improvements Plan

## Executive Summary

This document outlines architectural improvements based on production-readiness review. The current implementation is **strong** with excellent LangGraph usage, deterministic routing, and LLM-as-Judge validation. The primary improvement needed is moving from **direct LLM SQL generation** to **SQL Intent pattern** for enhanced safety and maintainability.

---

## âœ… What's Working Exceptionally Well (DO NOT CHANGE)

### 1. LangGraph Implementation
- âœ… Correct use of stateful DAG
- âœ… Clean node separation
- âœ… Bounded retry loop
- âœ… Graph compiled once (singleton) â†’ performance optimized

### 2. Deterministic Router
- âœ… Explainable, testable, debuggable
- âœ… No LLM-based routing (avoids common mistake)
- âœ… Enterprise-grade approach

### 3. LLM-as-Judge
- âœ… Fact consistency checking
- âœ… Citation coverage validation
- âœ… Scope leakage detection
- âœ… Confidence scoring

### 4. Observability & Debug Trace
- âœ… Node-level latency tracking
- âœ… Tool usage logging
- âœ… Router decisions captured
- âœ… Judge verdicts recorded
- âœ… Can answer "why did the agent say this?"

---

## âš ï¸ Critical Improvement: SQL Intent Pattern

### Current Risk: Dynamic SQL Generation

**Problem:**
- LLM generates raw SQL, then validates it
- Schema drift â†’ silent failures
- Prompt injection via user query
- JOIN hallucinations
- Hard-to-reproduce bugs
- Security audits will flag this

**Solution: Move from "LLM SQL" â†’ "LLM SQL Intent"**

### Implementation Plan

#### Step 1: Add SQLIntent Schema âœ… (Created)

```python
class SQLIntent(BaseModel):
    operation: Literal["count", "sum", "avg", "list", "group_by"]
    table: Literal["experiments", "samples", ...]
    filters: Dict[str, Any]
    group_by: Optional[List[str]]
    order_by: Optional[List[Dict[str, str]]]
    limit: Optional[int]
    time_range: Optional[Dict[str, str]]
    select_columns: Optional[List[str]]
```

#### Step 2: Update SQL Service

**Current Flow:**
```
Query â†’ LLM â†’ Raw SQL â†’ Validate â†’ Execute
```

**New Flow:**
```
Query â†’ LLM â†’ SQLIntent â†’ Template Selection â†’ Safe SQL â†’ Execute
```

**Benefits:**
- âœ… No hallucinated JOINs
- âœ… Full control over SQL structure
- âœ… Easy unit tests
- âœ… Auditor-friendly
- âœ… Schema changes handled in code, not prompts

#### Step 3: SQL Template Library

Create template functions for each operation:

```python
def build_count_query(intent: SQLIntent, scope: Dict) -> str:
    """Build COUNT query from intent."""
    # Template-based, safe SQL generation
    pass

def build_list_query(intent: SQLIntent, scope: Dict) -> str:
    """Build SELECT query from intent."""
    pass

def build_group_by_query(intent: SQLIntent, scope: Dict) -> str:
    """Build GROUP BY query from intent."""
    pass
```

#### Step 4: Update LLM Prompt

**Old Prompt:**
```
"Generate a PostgreSQL SELECT query..."
```

**New Prompt:**
```
"Generate a structured SQL intent JSON. Do NOT write SQL.
Map the user query to:
- operation: count|sum|avg|list|group_by
- table: experiments|samples|...
- filters: {status: 'in_progress', ...}
- group_by: ['status'] (if needed)
..."
```

---

## ðŸ”§ System Prompt Improvements

### 4.1 Normalizer Prompt - Tighten Intent Taxonomy

**Current:**
```python
intent: "aggregate" | "search" | "hybrid"
```

**Recommended:**
```python
intent:
- "quantitative"          # Counts, sums, averages
- "qualitative"           # Semantic search, descriptions
- "quantitative_with_explanation"  # Numbers + context
- "comparison"            # Compare entities
- "anomaly_detection"     # Find outliers
```

**Benefits:**
- Router logic becomes clearer
- Judge reasoning improves
- Future agents (statistics, forecasting) plug in cleanly

### 4.2 SQL Prompt - Remove Schema Dump

**Current:**
- Full DB schema in prompt (274 lines)
- Increases token cost
- Breaks when schema grows

**Recommended:**
```
Allowed tables:
experiments(id, status, project_id, organization_id, created_at)
samples(id, sample_type, status, experiment_id, ...)
...

Relationships:
experiments.project_id â†’ projects.id
projects.organization_id â†’ organizations.id
```

**Benefits:**
- Reduced token cost
- Easier to maintain
- Schema knowledge in code, not prompts

### 4.3 Summarizer Prompt - Force Uncertainty

**Add Rule:**
```
If evidence is insufficient, explicitly say "insufficient evidence".
Do NOT speculate.
```

**Benefits:**
- Reduces confident hallucinations
- Better user experience
- Judge can catch overconfidence

---

## ðŸ”„ Retry Logic Enhancement

### Current Behavior
```
Judge fails â†’ Retry â†’ Router again â†’ Full re-execution
```

### Improved Behavior
```
Judge fails â†’ Retry â†’ Inject judge issues â†’ Summarizer refines only answer
```

**Implementation:**
```python
# In retry_node
state["retry_context"] = {
    "judge_issues": state["judge"]["issues"],
    "suggested_revision": state["judge"].get("suggested_revision")
}

# In summarizer_node
if state.get("retry_context"):
    # Use existing SQL/RAG results
    # Only refine the answer based on judge feedback
```

**Benefits:**
- Avoids unnecessary SQL/RAG reruns
- Faster retries
- Lower cost
- Better user experience

---

## ðŸ“Š RAG Layer Enhancement

### Current (Strong)
- âœ… Similarity threshold (0.75)
- âœ… Dedup by experiment_id
- âœ… Scope filtering
- âœ… Top-k cap (6)

### Future Enhancement: Citation Grounding Score

**Add:**
```python
def calculate_citation_grounding(answer: str, citations: List[Citation]) -> float:
    """
    Calculate % of answer sentences backed by citations.
    
    Returns:
        0.0-1.0 score indicating how well-grounded the answer is
    """
    # Parse answer into sentences
    # Check each sentence has citation
    # Return percentage
```

**Feed to Judge:**
```python
grounding_score = calculate_citation_grounding(summary["answer"], summary["citations"])
judge_input["citation_grounding"] = grounding_score
```

**Benefits:**
- Better confidence calibration
- Judge can catch unsupported claims
- Improves answer quality

---

## ðŸ“ˆ Confidence Score Calibration

### Current
- Some heuristic boosts
- Judge confidence used
- Fine for initial version

### Future: Calibration System

**Track:**
- Overconfidence vs underconfidence
- Calibrate against golden set
- A/B test different confidence formulas

**Implementation:**
```python
class ConfidenceCalibrator:
    """Calibrate confidence scores against ground truth."""
    
    def calibrate(self, predicted: float, actual: bool) -> float:
        """Adjust confidence based on historical accuracy."""
        pass
    
    def track_prediction(self, confidence: float, was_correct: bool):
        """Track prediction for calibration."""
        pass
```

**Benefits:**
- Confidence becomes a product feature
- Better user trust
- Data-driven improvements

---

## ðŸ”’ Security & Compliance Enhancements

### Current (Strong)
- âœ… Tenant isolation
- âœ… Read-only SQL
- âœ… Scope enforcement
- âœ… Vector filtering

### Future Additions

#### 1. Prompt Injection Detection
```python
def detect_prompt_injection(text: str) -> bool:
    """Detect potential prompt injection in user queries."""
    # Check for SQL keywords in unexpected places
    # Check for instruction-like patterns
    # Check for encoding tricks
    pass
```

#### 2. Audit Log Persistence
```python
class AuditLogger:
    """Immutable audit log for compliance."""
    
    def log_query(self, request: AgentRequest, response: FinalResponse):
        """Log all queries and responses."""
        # Store in immutable storage
        # Include: user_id, query, response, confidence, tool_used
        pass
```

**Benefits:**
- Compliance ready
- Security incident investigation
- User behavior analysis

---

## ðŸ“‹ Implementation Priority

### Phase 1: Critical (Immediate)
1. âœ… **SQL Intent Pattern** - Highest priority
   - Create SQLIntent schema
   - Update SQL service to use templates
   - Update LLM prompt to generate intent, not SQL

### Phase 2: High (Next Sprint)
2. **Retry Logic Enhancement**
   - Add retry_context to state
   - Update summarizer to use existing results
   
3. **Normalizer Intent Taxonomy**
   - Expand intent types
   - Update router logic

### Phase 3: Medium (Future)
4. **System Prompt Optimization**
   - Reduce schema dump
   - Add uncertainty enforcement
   
5. **Citation Grounding Score**
   - Implement calculation
   - Feed to judge

### Phase 4: Nice to Have
6. **Confidence Calibration**
   - Build calibration system
   - Track predictions
   
7. **Security Enhancements**
   - Prompt injection detection
   - Audit log persistence

---

## ðŸŽ¯ Success Metrics

### SQL Intent Pattern
- âœ… Zero SQL injection vulnerabilities
- âœ… 100% test coverage of SQL templates
- âœ… Reduced token cost (no schema dump)
- âœ… Faster SQL generation (intent â†’ template is faster)

### Retry Logic
- âœ… 50% reduction in retry latency
- âœ… 30% reduction in LLM calls during retries

### Confidence Calibration
- âœ… Confidence scores within 10% of actual accuracy
- âœ… User trust metrics improve

---

## ðŸ“š References

- SQL Intent Pattern: Inspired by LangChain's SQL agent improvements
- Confidence Calibration: "Predictive Uncertainty Quantification" (Guo et al., 2017)
- Prompt Injection: OWASP LLM Top 10

---

## âœ… Next Steps

1. **Review this document** with team
2. **Prioritize Phase 1** (SQL Intent Pattern)
3. **Create implementation tickets**
4. **Set up monitoring** for confidence scores
5. **Plan security audit** after SQL Intent implementation

---

**Last Updated:** 2025-01-20
**Status:** Planning Phase
**Owner:** Engineering Team
